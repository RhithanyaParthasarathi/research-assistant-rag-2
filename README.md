# ğŸ“š Research Assistant (FastAPI + Streamlit + LangChain + Gemini + Qdrant)

An **AI-powered research assistant** built with **FastAPI (backend)** and **Streamlit (frontend)**.  
Upload your documents, chat with them, and get contextual answers powered by **Google Gemini** + **LangChain** + **Qdrant**.

## ğŸš€ Features
- ğŸ“„ Upload and process documents (`.pdf`, `.docx`, `.pptx`)  
- ğŸ” Semantic search with **Qdrant Vector DB**  
- ğŸ¤– Conversational AI with **LangChain + Google Gemini**  
- âš¡ REST API with FastAPI & interactive UI with Streamlit  
- â˜ï¸ Deployable on **Render / Railway / Streamlit Cloud**  


## ğŸ“‚ Project Structure
research-assistant-rag-2/
â”‚â”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI backend application
â”‚   â”œâ”€â”€ requirements.txt     # Backend dependencies
â”‚
â”‚â”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py               # Streamlit frontend application
â”‚   â”œâ”€â”€ requirements.txt     # Frontend dependencies
â”‚
â”‚â”€â”€ app.py                   # (root app.py, optional/legacy)
â”‚â”€â”€ ingest.py                # Script for ingesting documents
â”‚â”€â”€ requirements.txt         # Combined/global dependencies
â”‚â”€â”€ .gitignore

Installation
Clone the repository
git clone https://github.com/RhithanyaParthasarathi/research-assistant-rag-2?authuser=0
cd research-assistant-rag-2

Setup Backend (FastAPI)
cd backend
python -m venv .venv
source .venv/bin/activate   # On Windows: .venv\Scripts\activate
pip install -r requirements.txt


Run backend:

uvicorn main:app --host 0.0.0.0 --port 8080 --reload

Setup Frontend (Streamlit)

In a new terminal:

cd frontend
python -m venv .venv
source .venv/bin/activate   # On Windows: .venv\Scripts\activate
pip install -r requirements.txt

Run frontend:

streamlit run app.py
